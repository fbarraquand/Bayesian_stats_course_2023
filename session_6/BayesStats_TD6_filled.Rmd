---
title: "Analysis of Swiss Crested Tits population dynamics III"
author: "F. Barraquand - adapted from Kéry and Royle's Applied hierarchical modeling in Ecology"
date: "December 5, 2023"
output:
  html_document:
    highlight: textmate
    theme: united
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
options(width = 300)
knitr::opts_chunk$set(cache = FALSE) 
library(R2jags)
library(knitr)
```

```{r details-about-code, include=FALSE,eval=FALSE}
#  --------------- Original source: ---------------------------------------  
#   Applied hierarchical modeling in ecology
#   Modeling distribution, abundance and species richness using R and BUGS
#   Volume 2: Dynamic and Advanced models
#   Marc Kéry & J. Andy Royle
# Chapter 1 : RELATIVE ABUNDANCE MODELS FOR POPULATION DYNAMICS
# =============================================================
# Code from proofs dated 2020-08-18
# see https://github.com/mikemeredith/AHM_code
#  ---------------modified by FB starting 2021-05-23 ----------------------  
``` 

# Generalized linear mixed models

We have two reasons to switch to generalized linear (mixed) models:

* residuals for the LMM are, as we have seen, OK but not perfect
* our dataset is for the moment restricted to sites that have a lot of observations. What ensures to us that the increasing trends, for instance, maintain once we include sites that have very few birds (perhaps there's an increasing trend in sites with lots of birds already, and decreasing trends in sites with few birds!). Let's check that. 

## Variance partitioning bis repetita

We will now fit the model 

$$ Y_{it} \sim \mathcal{P}(\lambda_{it}), \; \ln(\lambda_{it}) = \mu + A_i + B_t +\epsilon_{it}$$
with $A_i \sim \mathcal{N}(0,\sigma_{\text{site}}^2)$ and $B_t ~ \sim \mathcal{N}(0,\sigma_{\text{year}}^2)$, as well as $\epsilon_{it} \sim \mathcal{N}(0,\sigma^2)$. 

The last random variate is not strictly needed for variance partitioning but is convenient here for two reasons:

* It represents the more-than-Poisson variance that is neither site variance nor year variance
* The Poisson log-normal distribution allows to model more skewed distribution than Poisson. 


We re-execute the previous code to load the data (all data) and get the covariates

```{r load-and-plot-data}
dat<-read.csv("../session_4/crestedTit.csv")
#str(dat)
dat$X<-NULL
# head(dat)
## Note: date991 is the date for year 99 and 1st survey, date992 second survey etc. 
C <- as.matrix(dat[,6:23]) # grab counts 1999:2016
#head(C)
year <- 1999:2016

```


```{r data-wrangling}
# Grab data for survey dates (for 2-3 surveys per year) and for duration
# Put into 3D array first, then summarize over reps within a year
nsite <- nrow(C)
nyear <- length(year)
datetmp <- as.matrix(dat[,24:77])
datefull <- array(datetmp, dim = c(nsite, 3, nyear))
durtmp <- as.matrix(dat[,78:131])
durfull <- array(durtmp, dim = c(nsite, 3, nyear))

# Get mean date of survey and mean survey duration for each site and year
date <- apply(datefull, c(1,3), mean, na.rm = TRUE)
dur <- apply(durfull, c(1,3), mean, na.rm = TRUE)
date[date == 'NaN'] <- NA
dur[dur == 'NaN'] <- NA
```

```{r format-covariates}
# Scale some covariates and mean-impute missing values in them
elev.sc <- as.vector(scale(dat$elev))       # elevation of site
forest.sc <- as.vector(scale(dat$forest))   # forest cover of site
date.sc <- scale(date)
date.sc[is.na(date.sc)] <- 0           # mean impute
dur.sc <- scale(dur)
dur.sc[is.na(dur.sc)] <- 0             # mean impute

# Nota bene 1 : you have to set priors on covariates if you want to use covariates with NAs
# Nota bene 2 : If you want to infer missing values in the data, you set priors on C (the data itself, we haven't done this)

# Bundle and summarize data
M <- nrow(C)
T <- ncol(C)
str(bdata <- list(C = C, M = M, T = T) )
```


We now code the model (because of all sites, this make take longer)

```{r glm1,eval=FALSE,include=FALSE}
## This is something we could have done but won't do -- a GLM model, without random effects. Similar results, some sites effects won't be well estimated. 
cat(file = "model_glm1.txt","
    model {
    # Priors
    for(i in 1:M){
    site[i] ~ dnorm(0, 0.001) # Priors for site effects
    }
    year[1] <- 0 # Constraint on year effects
    for(t in 2:T){
    year[t] ~ dnorm(0, 0.001) # Priors for year effects 2:T
    }
    # Likelihood
    for (i in 1:M){
    for(t in 1:T){
    C[i,t] ~ dpois(lambda[i,t])
    log(lambda[i,t]) <- site[i] + year[t]
    }
    }
    # Derived quantities
    for(t in 1:T){
    popindex[t] <- sum(lambda[,t])
    }
    }
    ")

# Initial values
inits <- function() list(site = rnorm(nrow(C)), year = c(NA, rnorm(ncol(C)-1)))

# Parameters monitored
params <- c("site", "year", "popindex")

# MCMC settings
na <- 1000 ; ni <- 15000; nt <- 10 ; nb <- 5000 ; nc <- 3

# Call JAGS (ART 5 min), check convergence and summarize posteriors
out1 <- jags(bdata, inits, params, "model_glm1.txt", n.adapt = na, n.chains = nc,n.thin = nt, n.iter = ni, n.burnin = nb, parallel = TRUE)
op <- par(mfrow = c(3,3)) ; traceplot(out1) ### FB: (very) bad convergence for some sites!!
par(op)
print(out1, 2) 

```

Coding the GLMM

```{r glmm1}

cat(file = "model_glmm1.txt","
    model {
    
    ## Priors
    mu ~ dnorm(0,0.001)
    
    # Site
    for (i in 1:M){
      site[i] ~ dnorm(0,tau.site)
    }
    tau.site <-pow(sd.site,-2)
    sd.site ~ dunif(0,3)
    
    # Year
    for (t in 1:T){
      year[t] ~ dnorm(0,tau.year)
    }
    tau.year <-pow(sd.year,-2)
    sd.year ~ dunif(0,3)
    
    # Residual variation
    tau<-pow(sd,-2)
    sd ~ dunif(0,3)
    
    ## Likelihood
    for (i in 1:M){
      for (t in 1:T){
        C[i,t] ~ dpois(lambda[i,t])
        epsilon[i,t] ~ dnorm(0,tau)
        log(lambda[i,t])<- mu + site[i] + year[t] + epsilon[i,t]
      }
    }
    
    ## Derived quantities
    for(t in 1:T){
    popindex[t] <- sum(lambda[,t])
      }
    }
    ")

# Initial values
inits <- function() list(mu = rnorm(1), site = rnorm(M), year = rnorm(T),
    epsilon = array(1, dim=c(M, T)))

# Parameters monitored
params <- c("mu", "sd.site", "sd.year", "sd", "site", "year", "popindex")

# MCMC settings
na <- 1000 ; ni <- 15000 ; nt <- 10 ; nb <- 5000 ; nc <- 3

# Call JAGS (ART 4 min), check convergence and summarize posteriors
# out2 <- jags(bdata, inits, params, "model_glmm1.txt", n.adapt = na,    n.chains = nc,n.thin = nt, n.iter = ni, n.burnin = nb, parallel = TRUE)
out2 <- jags(bdata, inits, params, "model_glmm1.txt", n.chains = nc,n.thin = nt, n.iter = ni, n.burnin = nb)

op <- par(mfrow = c(3, 2)) 
par(op)
summary(out2)
print(out2)

library(ggmcmc)
S<-ggs(as.mcmc(out2))
ggs_density(S,family = "sd")
ggs_traceplot(S,family = "sd")

# jagsUI version
# out2$mean$sd^2
# out2$mean$sd.site^2
# out2$mean$sd.year^2
# total = out2$mean$sd^2+ out2$mean$sd.site^2+out2$mean$sd.year^2
# out2$mean$sd^2/total
# out2$mean$sd.site^2/total
# out2$mean$sd.year^2/total

out2$BUGSoutput$mean$sd^2
out2$BUGSoutput$mean$sd.site^2
out2$BUGSoutput$mean$sd.year^2
total = out2$BUGSoutput$mean$sd^2+ out2$BUGSoutput$mean$sd.site^2+out2$BUGSoutput$mean$sd.year^2
out2$BUGSoutput$mean$sd^2/total
out2$BUGSoutput$mean$sd.site^2/total
out2$BUGSoutput$mean$sd.year^2/total



```

[check effects of estimates - do they change much compared to previous LMMs?]

98% of the variation is now due to the sites. Is that a large change? If so what could it be due to? Different model or different data set. 

Let's try to select the previous 97 sites to compare.  

```{r selecting sites}
# Check number of zero years in all sites
nzero <- apply(C, 1, function(x) sum(x == 0, na.rm = TRUE))
plot(sort(nzero)) # Make a graph of number of zero years
sum(nzero <= 1)   # 97 sites with at most 1 zero year
table(nzero)

# Bundle data with restriction on sites
sel <- nzero <= 1 # Select sites with <= 1 zero count
newM <- sum(sel)  # Define new number of sites
str(bdata <- list(C = C[sel,], M = newM, T = ncol(C[sel,])))
# List of 3
# $ C: int [1:97, 1:18] 3 5 9 13 12 2 3 3 6 10 ...
# $ M: int 97
# $ T: int 18

```


```{r glmm1-bis}

inits <- function() list(site = rnorm(nrow(C[sel,])), year = c(NA, rnorm(ncol(C[sel,])-1)))

# Call JAGS (ART 4 min), check convergence and summarize posteriors
#out2 <- jags(bdata, inits, params, "model_glmm1.txt", n.adapt = na, n.chains = nc,    n.thin = nt, n.iter = ni, n.burnin = nb, parallel = TRUE)
out2 <- jags(bdata, inits, params, "model_glmm1.txt", n.chains = nc,
    n.thin = nt, n.iter = ni, n.burnin = nb)
op <- par(mfrow = c(3, 2)) 
par(op)
summary(out2)
print(out2)

# jagsUI version
# out2$mean$sd^2
# out2$mean$sd.site^2
# out2$mean$sd.year^2
# total = out2$mean$sd^2+ out2$mean$sd.site^2+out2$mean$sd.year^2
# out2$mean$sd^2/total
# out2$mean$sd.site^2/total
# out2$mean$sd.year^2/total

out2$BUGSoutput$mean$sd^2
out2$BUGSoutput$mean$sd.site^2
out2$BUGSoutput$mean$sd.year^2
total = out2$BUGSoutput$mean$sd^2+ out2$BUGSoutput$mean$sd.site^2+out2$BUGSoutput$mean$sd.year^2
out2$BUGSoutput$mean$sd^2/total
out2$BUGSoutput$mean$sd.site^2/total
out2$BUGSoutput$mean$sd.year^2/total

```

Now we've got 78% of the variance explained by the site. A lot of the change in the percentage of explained variance can be explained by the change in model, although changing the dataset for the full dataset contributes too. 

## "Complete" previous model, Poisson log-normal version. 

[TODO by the students if possible]

```{r glmm2}
# Bundle and summarize data
str(bdata <- list(C = C, yr = year - mean(year), elev = elev.sc,
    forest = forest.sc, date = date.sc, dur = dur.sc,
    twosurveys = as.numeric(dat$nsurveys == 2), M = M, T = T) )

cat(file = "model_glmm2.txt","
model {

  # Priors and linear models
  mu ~ dnorm(0, 0.1)               # Grand mean (intercept)

  for (i in 1:M){
    site[i] ~ dnorm(0, tau.site)   # Random site effects
  }

  # Linear model for effect of elevation on expectation of trends
  for(i in 1:M){ # NOTE: here we model the trends
    gamma[i] ~ dnorm(mu.gamma[i], tau.gamma) # Random site-level trends
    mu.gamma[i] <- alpha.mu.gamma + beta1.mu.gamma * elev[i] +
        beta2.mu.gamma * pow(elev[i],2)
  }
  alpha.mu.gamma ~ dnorm(0, 0.1)   # intercept of mean trend on elev
  beta1.mu.gamma ~ dnorm(0, 0.1)   # lin effect of elev on trend
  beta2.mu.gamma ~ dnorm(0, 0.1)   # quad effect of elev on trend
  tau.gamma <- pow(sd.gamma, -2)
  sd.gamma ~ dunif(0, 0.2)         # Variability of trends

  # Other priors
  tau.site <- pow(sd.site, -2)
  sd.site ~ dunif(0, 3)

  # Theta priors 
  for (i in 1:7){
    theta[i] ~ dnorm(0, 0.1)       # Covariate effects
  }

  for(t in 1:T){
    year[t] ~ dnorm(0, tau.year)   # Random year effects
  }
  tau.year <- pow(sd.year, -2)
  sd.year ~ dunif(0, 2)

  # residual priors
  tau <- pow(sd, -2)
  sd ~ dunif(0, 1)

  # Likelihood
  for (i in 1:M){
    for(t in 1:T){
      C[i,t] ~ dpois(lambda[i,t])
      log(lambda[i,t]) <- mu + gamma[i] * yr[t] +
          theta[1] * elev[i] + theta[2] * pow(elev[i],2) +
          theta[3] * forest[i] + theta[4] * date[i,t] +
          theta[5] * pow(date[i,t],2) + theta[6] * dur[i,t] +
          theta[7] * twosurveys[i] + site[i] + year[t] + eps[i,t]
      eps[i,t] ~ dnorm(0, tau)
    }
  }
  # Derived quantities
  for(t in 1:T){
    popindex[t] <- sum(lambda[,t])
  }
}
")


# Initial values
inits <- function() list(mu = rnorm(1), gamma = rnorm(M), theta = rnorm(7),
    site = rnorm(M), year = rnorm(T), eps = array(1, dim=c(M, T)))

# Parameters monitored
params <- c("mu", "alpha.mu.gamma", "beta1.mu.gamma", "beta2.mu.gamma",
    "sd.beta", "theta", "sd.site", "sd.year", "sd", "popindex")
# could also monitor some random effects: "gamma", "site", "year",

# MCMC settings
na <- 5000 ; ni <- 10000 ; nt <- 5 ; nb <- 5000 ; nc <- 3

# Call JAGS (ART 7 min), check convergence and summarize posteriors
#out4 <- jags(bdata, inits, params, "model_glmm2.txt", n.adapt = na, n.chains = nc,    n.thin = nt, n.iter = ni, n.burnin = nb, parallel = TRUE)
out4 <- jags(bdata, inits, params, "model_glmm2.txt", n.chains = nc,    n.thin = nt, n.iter = ni, n.burnin = nb)
#par(mfrow = c(3,2)) ; traceplot(out4) ; par(mfrow = c(1,1))
print(out4, 2)

par(mfrow=c(2,1))
# 
# effect_elev = function(x){return(out4$mean$ beta1.mu.gamma*x+out4$mean$beta2.mu.gamma*x^2)}
# curve(effect_elev,from=min(elev.sc),to=max(elev.sc),xlab="Stdized elevation",ylab="effect on temporal trend")
# 
# # do the other effects on abundance stay similar? 
# effect_elev_trend = function(x){return(out4$mean$theta[1]*x+out4$mean$theta[2]*x^2)}
# curve(effect_elev_trend,from=min(elev.sc),to=max(elev.sc),xlab="Stdized elevation",ylab="effect on abundance")

effect_elev = function(x){return(out4$BUGSoutput$mean$ beta1.mu.gamma*x+out4$BUGSoutput$mean$beta2.mu.gamma*x^2)}
curve(effect_elev,from=min(elev.sc),to=max(elev.sc),xlab="Stdized elevation",ylab="effect on temporal trend")

# do the other effects on abundance stay similar? 
effect_elev_trend = function(x){return(out4$BUGSoutput$mean$theta[1]*x+out4$BUGSoutput$mean$theta[2]*x^2)}
curve(effect_elev_trend,from=min(elev.sc),to=max(elev.sc),xlab="Stdized elevation",ylab="effect on abundance")


par(mfrow=c(1,1))
```

[check effects of estimates - do they change? Still negligible effects of dur and twosurveys?]

Twosurveys now has a strong effect, since we include poorly surveyed sites. The other effects do not truly change, which is nice! 

```{r explained-variation}
# Measures by Kéry and Royle -- how much do the extra effects reduce the estimated SDs of year and site effects
# (shouldn't this be measured on the variance scale though? how legit is this?)
#(R2site <- 100* (out2$mean$sd.site - out4$mean$sd.site) / out2$mean$sd.site)
#(R2year <- 100* (out2$mean$sd.year - out4$mean$sd.year) / out2$mean$sd.year)
# (R2resi <- 100* (out2$mean$sd - out4$mean$sd) / out2$mean$sd)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Found by Kéry and Royle
# [1] 49.35703
# [1] 10.07
# [1] 31.38612

```




