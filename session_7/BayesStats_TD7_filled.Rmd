---
title: "Analyses of piracy (kleptoparasitism) in salmon-eating Bald eagles"
author: "F. Barraquand - adapted from Mc Elreath's Statistical Rethinking"
date: "December 12, 2023"
output:
  html_document:
    highlight: textmate
    theme: united
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
options(width = 300)
knitr::opts_chunk$set(cache = FALSE) 
library(R2jags)
library(knitr)
library(mcmcplots)
library(MASS)
```

# Binomial GLM

The course part does logistic regression. Now we do something akin to an ANOVA, but with binary data. 


```{r eagles-exploring data}
data(eagles)
eagles
#P // Size of pirating eagle (L = large, S = small).
#A // Age of pirating eagle (I = immature, A = adult).
#V // Size of victim eagle (L = large, S = small).
 
eagles.data = list(N=nrow(eagles),y=eagles$y,z=eagles$n,
               P=as.numeric(eagles$P)-1,A=as.numeric(eagles$A)-1,
               V=as.numeric(eagles$V)-1)

```


We're in a similar situation to the turtles sex, but with categorical explanatory variables

```{r fitting the first model}

cat(file = "eagles.glm.txt","
model {
  # Priors
  beta_0 ~ dnorm(0,0.5) # Reference combination P = L, A = A, V = L
  beta_P ~ dnorm(0,0.5) # Effect P: L->S
  beta_A ~ dnorm(0,0.5) # Effect A: A->I
  beta_V ~ dnorm(0,0.5) # Effect V: L->S

  # Likelihood
  for (i in 1:N){
  y[i] ~ dbin(p[i],z[i])
  logit(p[i])<-beta_0 + beta_P*P[i] + beta_A*A[i] + beta_V*V[i]
  }
  
  # Derived quantities
  Prob_0 <- 1/(1+exp(-beta_0))
  Prob_P <- 1/(1+exp(-beta_0-beta_P))
  Prob_A <- 1/(1+exp(-beta_0-beta_A))
  Prob_V <- 1/(1+exp(-beta_0-beta_V))
  Ratio_P<- Prob_P/Prob_0
  Ratio_A<- Prob_A/Prob_0
  Ratio_V<- Prob_V/Prob_0
  
  # Does experience replace size/strength?
  Ratio_hyp <- Prob_P/Prob_A
  
}
")

```

Fitting the model

```{r fitting-glm}
# Initial values
inits <-function(){list(beta_0=rnorm(1,0,1),beta_P=rnorm(1,0,1),beta_A=rnorm(1,0,1),beta_V=rnorm(1,0,1))}
# Parameters to estimate
params = c("beta_0","beta_P","beta_A","beta_V","Prob_0","Prob_P","Prob_A","Prob_V","Ratio_P","Ratio_A","Ratio_V","Ratio_hyp")

# MCMC settings
nc <- 3  ;  ni <- 2000  ;  nb <- 1000  ;  nt <- 2
# Call JAGS, check convergence and summarize posteriors
out <- jags(eagles.data, inits, params, "eagles.glm.txt",
            n.thin = nt, n.chains = nc, n.burnin = nb, n.iter = ni)
print(out, dig = 3)     # Bayesian analysis
```

Model checking

```{r checking}
traplot(out)
```

```{r running-jagsUI}
library(jagsUI)
out <- jagsUI::jags(eagles.data, inits, params, "eagles.glm.txt",
            n.thin = nt, n.chains = nc, n.burnin = nb, n.iter = ni)
print(out, dig = 3)  
traceplot(out)
```


## Interpreting the model

We see the following effects: 
- 

- we see that size is more important than experience. 

## A model with hierarchical priors (hyperprior)

Next put a hyperprior on $\beta$s. What are we transforming this model into? 

```{r new-model}

```

